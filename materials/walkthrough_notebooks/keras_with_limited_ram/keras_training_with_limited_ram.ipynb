{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E9wN1a_EUHv"
      },
      "source": [
        "### Training NLP models in Colab without running out of RAM\n",
        "\n",
        "This notebook focuses on some techniques you can use to avoid running out of memory, when working with a lot of data and large models used for NLP tasks.\n",
        "\n",
        "The task we'll work on is textual entailment, using the [Stanford Natural Language Inference (SNLI) dataset](https://nlp.stanford.edu/projects/snli/). We'll build a fairly simple classification model, using a pre-trained BERT model. (Some of the code is inspired by [this Keras example for SNLI classification](https://keras.io/examples/nlp/semantic_similarity_with_bert/).)\n",
        "\n",
        "The main focus of this notebook is not on the task or model architecture, but on how to load part of your data at a time while you train, and save model checkpoints as you go. You should be able to run the notebook on the free tier of Google Colab. (There is a point where it will run out of RAM, for demonstration, but that is noted in the comments.)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2024-summer-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/keras_training_with_limited_ram.ipynb)\n"
      ],
      "id": "4E9wN1a_EUHv"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTb7WQBJETaa",
        "outputId": "bd978d44-4084-4c5d-d03a-be509e27d7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.37.2"
      ],
      "id": "mTb7WQBJETaa"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GgTstPMhUCcB"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# These auto classes load the right type of tokenizer and model based on a model name\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "id": "GgTstPMhUCcB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIoEALljGPSb"
      },
      "source": [
        "We'll start by downloading the data, using curl in bash to save it to the local disk space for the Colab notebook. You might have your data in Google Drive instead; later we'll mount a Drive folder to this notebook so that we can save our model someplace more permanent, but you can move that step up if you need to load data from Drive."
      ],
      "id": "cIoEALljGPSb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98eb0b67",
        "outputId": "1ea9b19b-4443-4e91-83d2-face4682be63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11.1M  100 11.1M    0     0  7250k      0  0:00:01  0:00:01 --:--:-- 7246k\n",
            "SNLI_Corpus/\n",
            "SNLI_Corpus/snli_1.0_dev.csv\n",
            "SNLI_Corpus/snli_1.0_train.csv\n",
            "SNLI_Corpus/snli_1.0_test.csv\n"
          ]
        }
      ],
      "source": [
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz"
      ],
      "id": "98eb0b67"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "084fa28f",
        "outputId": "9008a8cb-cca5-4d34-d216-e6edcb13ab9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "snli_1.0_dev.csv  snli_1.0_test.csv  snli_1.0_train.csv\n"
          ]
        }
      ],
      "source": [
        "!ls SNLI_Corpus"
      ],
      "id": "084fa28f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXir3qflG6mf"
      },
      "source": [
        "First let's read in the entire train and dev datasets. It looks like we have about 550k training examples, and 10k dev examples (which we'll use for validation). Just loading those short sentence pairs doesn't take a lot of RAM, but it will be too much to process with a BERT model. (You can see how much RAM and Disk space you're using by looking in the upper right corner of the notebook.)"
      ],
      "id": "OXir3qflG6mf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGgVdcybUQM7",
        "outputId": "673a539b-af5a-4ef9-e6ad-e112ca83d140"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((550152, 3), (10000, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_filename = 'SNLI_Corpus/snli_1.0_train.csv'\n",
        "dev_filename = 'SNLI_Corpus/snli_1.0_dev.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_filename)\n",
        "df_dev = pd.read_csv(dev_filename)\n",
        "\n",
        "df_train.shape, df_dev.shape"
      ],
      "id": "NGgVdcybUQM7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCbmUSvNHU83"
      },
      "source": [
        "Let's define some functions that we'll need to preprocess the data and build our classification model. First, we'll tokenize the sentence pairs using the pretrained BERT tokenizer. Second, we need to convert the three label classes from strings to numeric values."
      ],
      "id": "vCbmUSvNHU83"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sz6xMq-CdF2V"
      },
      "outputs": [],
      "source": [
        "label_dict = {'neutral': 0, 'entailment': 1, 'contradiction': 2}"
      ],
      "id": "Sz6xMq-CdF2V"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "00bee460"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(sentence_pairs, label_strs, tokenizer, max_length=128):\n",
        "    # With BERT tokenizer's batch_encode_plus, sentence pairs are\n",
        "    # encoded together and separated by [SEP] token.\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        sentence_pairs,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    # Extract encoded features and labels, add to corresponding lists\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "    # Convert string labels into numbered categories\n",
        "    labels = np.array([label_dict[label] if label in label_dict else 0\n",
        "                       for label in label_strs])\n",
        "\n",
        "    return [input_ids, attention_masks, token_type_ids], labels"
      ],
      "id": "00bee460"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqVIMi8CqJ2N"
      },
      "source": [
        "For the model, we'll construct a fairly simple classification model on top of the pretrained BERT model. Since we're freezing the full BERT model, it doesn't work very well for this classification problem to just use the pre-trained CLS token output as our vector representing the full input that we want to classify. (It would probably work better if we unfroze some BERT layers to fine-tune that CLS token.) Instead, we'll add one more attention layer on top of the full sequence of contextualized token vectors that we get out of BERT, so that we can train that attention layer to pay attention to the tokens that are most useful for this entailment task."
      ],
      "id": "OqVIMi8CqJ2N"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "336d30d4"
      },
      "outputs": [],
      "source": [
        "def build_snli_model(bert_model, max_length=128, hidden_dim=256):\n",
        "    # Freeze all layers of the BERT model except the last layer\n",
        "    for w in bert_model.weights:\n",
        "        if not 'layer_._11' in w.name:\n",
        "            w._trainable = False\n",
        "\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_masks')\n",
        "    token_type_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='token_type_ids')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "\n",
        "    bert_output = bert_model(bert_inputs)\n",
        "    cls_output = bert_output[0][:, 0, :]\n",
        "\n",
        "    dropout_output = layers.Dropout(0.3)(cls_output)\n",
        "    final_output = layers.Dense(3, activation=\"softmax\")(dropout_output)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, token_type_ids],\n",
        "                                  outputs=[final_output])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "id": "336d30d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhUr-JYBH26m"
      },
      "source": [
        "Ok, let's load the pretrained tokenizer and model, build the classification model, and preprocess our data to get ready to train. We'll freeze the BERT model layers in this example, so we keep the pre-trained weights rather than fine-tuning. (We will still be training the new layers we add on top of BERT for classification.) If you set the last line in the cell below to True, and train further on your task, you'll be fine-tuning the BERT model. It will take longer to train."
      ],
      "id": "NhUr-JYBH26m"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82de9daa",
        "outputId": "31818a52-a3a2-40a2-aa8e-f5c76f6e7eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_model_name='bert-base-uncased'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = TFBertModel.from_pretrained(bert_model_name)"
      ],
      "id": "82de9daa"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u181p__jMYz",
        "outputId": "bf6cde06-6c60-44b0-e478-df8af10edbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " attention_masks (InputLaye  [(None, 128)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf_bert_model_3 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['attention_masks[0][0]',     \n",
            " el)                         ngAndCrossAttentions(last_   40         'input_ids[0][0]',           \n",
            "                             hidden_state=(None, 128, 7              'token_type_ids[0][0]']      \n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 768)                  0         ['tf_bert_model_3[0][0]']     \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " dropout_150 (Dropout)       (None, 768)                  0         ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 3)                    2307      ['dropout_150[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109484547 (417.65 MB)\n",
            "Trainable params: 109484547 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_snli_model(bert_model, max_length=128, hidden_dim=256)\n",
        "model.summary()"
      ],
      "id": "9u181p__jMYz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEeC8rEi2lM6"
      },
      "source": [
        "Here's the code to run our preprocess_data function (tokenizing the text into vocab_ids) on the dev set and then the full train set. Note that your notebook might crash if you try to tokenize the full dataset, so only run that code if you want to try and see what happens."
      ],
      "id": "PEeC8rEi2lM6"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MaNJK72fa9m1"
      },
      "outputs": [],
      "source": [
        "# Tokenize the 1k of the dev examples to use for validation data first\n",
        "# (You can use more, 1k works well for a quick example)\n",
        "\n",
        "dev_sentence_pairs = df_dev[['sentence1', 'sentence2']].values[:1000].astype(str).tolist()\n",
        "dev_labels = df_dev['similarity'].values[:1000]\n",
        "\n",
        "dev_data = preprocess_data(\n",
        "    dev_sentence_pairs, dev_labels, tokenizer=bert_tokenizer, max_length=128\n",
        ")"
      ],
      "id": "MaNJK72fa9m1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8c98711"
      },
      "outputs": [],
      "source": [
        "# Now tokenize the 550k training examples ...\n",
        "# ONLY RUN THIS CELL THE FIRST TIME FOR DEMONSTRATION, IT MIGHT RUN OUT OF RAM\n",
        "\n",
        "sentence_pairs = df_train[['sentence1', 'sentence2']].values.astype(str).tolist()\n",
        "labels = df_train['similarity'].values\n",
        "\n",
        "train_data = preprocess_data(\n",
        "    sentence_pairs, labels, tokenizer=bert_tokenizer, max_length=128\n",
        ")"
      ],
      "id": "d8c98711"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHk4FBOWr3T"
      },
      "source": [
        "At some point when running the last cell, if you're using the free Collab tier, your notebook probably ran out of RAM and crashed. (If it didn't, you're on a Colab machine with more RAM; available resources may vary. But you may not be able to actually train the model with all of that data in memory.)\n",
        "\n",
        "Let's try again, but this time, we won't load all of our data at once. Connect the notebook again (it may have restarted on its own), and run most of the code above, but stop after tokenizing the dev data (which we'll keep for validation below). Don't tokenize the full 550k dataset."
      ],
      "id": "ZsHk4FBOWr3T"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QnoXcWoabhsU"
      },
      "outputs": [],
      "source": [
        "# In case you loaded the full dataset above\n",
        "df_train = None"
      ],
      "id": "QnoXcWoabhsU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyfiTM0LIUJt"
      },
      "source": [
        "We can define a [custom class called a data generator](https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3), that we will pass to model.fit instead of our full dataset. This data generator needs to implement methods for `__len__`, `__getitem__`, and `on_epoch_end`. In `__getitem__`, we'll write the code to get the next batch of data to train the model. We can write that function so it only loads and tokenizes the data needed for the next batch. In `on_epoch_end` we'll shuffle the order in which we plan to load data for the next epoch.\n",
        "\n",
        "We'll look at how to do this two ways. First, in the data we downloaded from SNLI, all of the training data is in one large CSV file. We can use the pandas `pd.read_csv` method, which includes options to skip certain rows of data and only load a certain number. We won't just want to load a consecutive chunk each time, because we'll want to shuffle the rows. The `read_csv` method doesn't quite have an option to specify individual row indices to load, but we can specify a list of row indices to skip, so that's what we'll do here."
      ],
      "id": "LyfiTM0LIUJt"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4b82e1a2"
      },
      "outputs": [],
      "source": [
        "class SNLIDataGeneratorFromFile(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=32,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        # return self.n_examples // self.batch_size\n",
        "        return 100\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "\n",
        "        sentence_pairs = df[['sentence1', 'sentence2']].values.astype(str).tolist()\n",
        "        labels = df['similarity'].values\n",
        "\n",
        "        batch_data = preprocess_data(\n",
        "            sentence_pairs,\n",
        "            labels,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "id": "4b82e1a2"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "os_CX135Yd3J"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our data generator, for our training data file and size\n",
        "\n",
        "train_data_generator = SNLIDataGeneratorFromFile(\n",
        "    tokenizer=bert_tokenizer,\n",
        "    n_examples=550152,\n",
        "    data_filename='SNLI_Corpus/snli_1.0_train.csv'\n",
        ")"
      ],
      "id": "os_CX135Yd3J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgtbBM7A-IZ"
      },
      "source": [
        "One more thing. It's going to take a while to train our model (even with a GPU, which we'll need to use). Colab resources are free but shared, there are usage limits and our notebook might time out especially when using a GPU for a while. So we should periodically save a copy of our trained model as we go. Later, we can load the model that we saved and keep training it further.\n",
        "\n",
        "At this point, we probably do want to mount a Google Drive folder, because we won't want to save our checkpoints just to temporary Colab disk space. If our notebook disconnects, we'll lose those files. The next cell mounts your Drive folder, then for demonstration I'm showing my (the instructor's) UC Berkeley Drive path to where I'm storing files for this semester's class. You'll want to edit that for your Drive."
      ],
      "id": "LQgtbBM7A-IZ"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wsjKJ7cA0-J",
        "outputId": "33fa8511-0938-4863-9a06-0298ecb6bfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "6wsjKJ7cA0-J"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fmI9rlUJA1bC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908b74a5-e64a-4732-a6c5-aa0fb66edab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t5base-finetuned-shakespeare-to-modern\tweights.02-0.54.hdf5  weights.05-0.54.hdf5\n",
            "t5_shakespeare_weights.01-0.85.hdf5\tweights.03-0.54.hdf5\n",
            "weights.01-0.53.hdf5\t\t\tweights.04-0.58.hdf5\n"
          ]
        }
      ],
      "source": [
        "# CHANGE THIS TO THE PATH IN YOUR OWN DRIVE WHERE YOU WANT TO SAVE CHECKPOINTS\n",
        "\n",
        "!ls drive/MyDrive/ISchool/MIDS/266/model_checkpoints/"
      ],
      "id": "fmI9rlUJA1bC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY4RwR5_syuG"
      },
      "source": [
        "Keras provides a handy [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/) class that we can pass into .fit as a callback. By default, it'll save a checkpoint of the model at the end of each epoch of training.\n",
        "\n",
        "We can choose to save the whole model or just the weights (i.e. the model parameters that we've trained so far). And we'll specify the destination filepath (we can include formatting options to have different filenames for each epoch and loss).\n",
        "\n",
        "(Other options: you can also choose to only save the best performing model each time, based on a performance metric you choose. Here we'll save after every epoch here so that you can see the resulting files as you go, but feel free to choose different options based on the ModelCheckpoint documentation if you prefer.)"
      ],
      "id": "TY4RwR5_syuG"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "yNjbSiWN_Xce"
      },
      "outputs": [],
      "source": [
        "# CHANGE checkpoint_dir TO THE PATH IN YOUR OWN DRIVE WHERE YOU WANT TO SAVE CHECKPOINTS\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 'weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ],
      "id": "yNjbSiWN_Xce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hCG0YzqKb2w"
      },
      "source": [
        "Now we're ready to train our model. We'll call `model.fit`, but instead of passing in an array of data, we'll pass in our data generator. And we'll include the model checkpoint callback, to save the weights after each epoch.\n",
        "\n",
        "The next cell may take a couple hours to run per epoch on the full dataset. The ETA in the running output can be very useful to estimate how long it will take your model to train (and whether you need to interrupt it and make adjustments to be able to make progress)."
      ],
      "id": "_hCG0YzqKb2w"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUv93OhYWe1Z",
        "outputId": "10b54f2d-92d1-4158-b741-5887cb01f214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_3/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_3/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_3/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_3/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_3/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 17/100 [====>.........................] - ETA: 48s - loss: 1.5493 - accuracy: 0.3419"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 58/100 [================>.............] - ETA: 23s - loss: 1.2387 - accuracy: 0.3685"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 79/100 [======================>.......] - ETA: 12s - loss: 1.1778 - accuracy: 0.4003"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 96/100 [===========================>..] - ETA: 2s - loss: 1.1361 - accuracy: 0.4281"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 89s 778ms/step - loss: 1.1276 - accuracy: 0.4338 - val_loss: 0.9130 - val_accuracy: 0.5730\n",
            "Epoch 2/5\n",
            " 70/100 [====================>.........] - ETA: 18s - loss: 0.8936 - accuracy: 0.5955"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 76/100 [=====================>........] - ETA: 14s - loss: 0.8868 - accuracy: 0.5995"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 81/100 [=======================>......] - ETA: 11s - loss: 0.8816 - accuracy: 0.6038"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 73s 728ms/step - loss: 0.8690 - accuracy: 0.6106 - val_loss: 0.7397 - val_accuracy: 0.7170\n",
            "Epoch 3/5\n",
            " 31/100 [========>.....................] - ETA: 43s - loss: 0.7747 - accuracy: 0.6764"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 37/100 [==========>...................] - ETA: 38s - loss: 0.7884 - accuracy: 0.6647"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 59/100 [================>.............] - ETA: 25s - loss: 0.7904 - accuracy: 0.6684"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 74s 736ms/step - loss: 0.7873 - accuracy: 0.6725 - val_loss: 0.6712 - val_accuracy: 0.7350\n",
            "Epoch 4/5\n",
            " 20/100 [=====>........................] - ETA: 51s - loss: 0.7737 - accuracy: 0.6438"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 74s 738ms/step - loss: 0.7415 - accuracy: 0.6737 - val_loss: 0.6315 - val_accuracy: 0.7380\n",
            "Epoch 5/5\n",
            " 46/100 [============>.................] - ETA: 34s - loss: 0.7090 - accuracy: 0.7058"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 76/100 [=====================>........] - ETA: 14s - loss: 0.7106 - accuracy: 0.7007"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 84/100 [========================>.....] - ETA: 9s - loss: 0.7024 - accuracy: 0.7065 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 73s 729ms/step - loss: 0.6965 - accuracy: 0.7084 - val_loss: 0.6490 - val_accuracy: 0.7360\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b8ccb22e2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "model.fit(train_data_generator, validation_data=dev_data, epochs=5,\n",
        "          callbacks=[model_checkpoint_callback])"
      ],
      "id": "GUv93OhYWe1Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3wuQDUfMbhs"
      },
      "source": [
        "If we need to pick up where we left off, we can load the last weights that we saved into our model and then call `model.fit` again. You'll have to edit the code line below to load the specific weight file you want to continue using.\n",
        "\n",
        "The example below shows a filename \"weights.05-0.75.hdf5\". You can see the formating options we used to define checkpoint_filepath two cells above. The .05 means it was saved after the 5th epoch of training, and the 0.75 means it had validation accuracy of 75%.\n",
        "\n",
        "Including the validation accuracy in the filename is a handy way to glance through your saved model checkpoints and make sure you're picking up using the weights that had the highest validation accuracy so far, not necessarily the last epoch."
      ],
      "id": "O3wuQDUfMbhs"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HEQ3GSc1WfZE"
      },
      "outputs": [],
      "source": [
        "# CHANGE checkpoint_filepath TO EXACT NAME OF A SAVED CHECKPOINT YOU WANT TO LOAD\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 'weights.05-0.74.hdf5'\n",
        "model.load_weights(checkpoint_filepath)"
      ],
      "id": "HEQ3GSc1WfZE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkf9re_jEFfg"
      },
      "source": [
        "Option 2: What if our data is stored in many small files? We might want to only load one file of data at a time, and randomly shuffle the order in which we load files from the data folder for each training epoch.\n",
        "\n",
        "Just to demonstrate that option, we'll simulate having our data in multiple files. We'll use the same dataset, but read the full dataset once and write it to a bunch of csv files of 256 rows each. (You won't typically do this if your data starts out all in one file, in which case you can just read select rows like we did above.)"
      ],
      "id": "Zkf9re_jEFfg"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "d9da19f8"
      },
      "outputs": [],
      "source": [
        "!mkdir SNLI_Corpus/train_files/"
      ],
      "id": "d9da19f8"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7cb18474"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(train_filename)\n",
        "for i in range(0, 550152, 256):\n",
        "    df_train[i:i+256].to_csv('SNLI_Corpus/train_files/train_data_%d.csv' % i, index=False)\n",
        "\n",
        "df_train = None"
      ],
      "id": "7cb18474"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d891751",
        "outputId": "fc7a12f5-9f44-4064-e747-c408ff942b63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2150"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "data_dir = 'SNLI_Corpus/train_files/'\n",
        "data_filenames = os.listdir(data_dir)\n",
        "len(data_filenames)"
      ],
      "id": "4d891751"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqGhJG3g9oLv"
      },
      "source": [
        "Now we have 2150 separate csv files of training data, and we'll only want to load one or a few at a time to train our model. Our data files have 256 rows in each, and we'll only use 32 examples per batch, so we'll usually only load one file at a time and tokenize part of it for the next batch of data.\n",
        "\n",
        "The code below will work whether your files are larger or smaller than one batch, though. We'll keep track of which rows we've already used from the current file and take the next rows for a new batch, so we might run past the current file and load another file to fill up the rest of the batch.\n",
        "\n",
        "In your own project, you might have data files that are smaller or larger, so we've made the code somewhat flexible so that you can see how to load just enough files to get the next batch of data that you need."
      ],
      "id": "lqGhJG3g9oLv"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5c004e85"
      },
      "outputs": [],
      "source": [
        "class SNLIDataGeneratorFromDir(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 data_dir,\n",
        "                 examples_per_file,\n",
        "                 max_length=128,\n",
        "                 batch_size=32,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.data_dir = data_dir\n",
        "        self.examples_per_file = examples_per_file\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.filename_order = os.listdir(self.data_dir)\n",
        "        self.next_file_i = 0\n",
        "        self.next_row_i = 0\n",
        "\n",
        "        # Call on_epoch_end to shuffle data at start\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        # return self.n_examples // self.batch_size\n",
        "        return 100\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        files_to_load = (self.batch_size // self.examples_per_file) + 1\n",
        "\n",
        "        sentence_pairs = []\n",
        "        labels = []\n",
        "\n",
        "        for file_i in range(self.next_file_i, self.next_file_i + files_to_load):\n",
        "            filepath = os.path.join(self.data_dir, self.filename_order[file_i])\n",
        "            df = pd.read_csv(filepath)\n",
        "            n_remaining = self.batch_size - len(sentence_pairs)\n",
        "\n",
        "            start = self.next_row_i\n",
        "            end = self.next_row_i + n_remaining\n",
        "            curr_sent_pairs = df[['sentence1', 'sentence2']].values[start:end]\n",
        "            sentence_pairs.extend(curr_sent_pairs.tolist())\n",
        "\n",
        "            curr_labels = df['similarity'].values[start:end]\n",
        "            labels.extend(curr_labels.tolist())\n",
        "\n",
        "            if end < len(df):\n",
        "                self.next_file_i = file_i\n",
        "                self.next_row_i = end\n",
        "            else:\n",
        "                self.next_file_i = file_i + 1\n",
        "                self.next_row_i = 0\n",
        "\n",
        "            if len(sentence_pairs) >= self.batch_size:\n",
        "                break\n",
        "\n",
        "        batch_data = preprocess_data(\n",
        "            sentence_pairs,\n",
        "            labels,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "        return batch_data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.next_file_i = 0\n",
        "        self.next_row_i = 0\n",
        "\n",
        "        if self.shuffle:\n",
        "            self.filename_order = np.random.permutation(self.filename_order)"
      ],
      "id": "5c004e85"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEZFAte64aJQ"
      },
      "source": [
        "The rest of the code is the same (with the extra \"examples_per_file\" paramter for this version of the data generator). Create a new data generator, and pass it into your model's .fit method. You can use the same checkpoint callback as in the first option, and load the last saved weights in the same way as well."
      ],
      "id": "AEZFAte64aJQ"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9lYIQ76mCjxY"
      },
      "outputs": [],
      "source": [
        "train_data_generator = SNLIDataGeneratorFromDir(\n",
        "    tokenizer=bert_tokenizer,\n",
        "    n_examples=550152,\n",
        "    data_dir=data_dir,\n",
        "    examples_per_file=256\n",
        ")"
      ],
      "id": "9lYIQ76mCjxY"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9hs1MsUCj8g",
        "outputId": "2a7504cf-00f6-4ced-fd3f-ed89f619ad52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 60/100 [=================>............] - ETA: 13s - loss: 0.7493 - accuracy: 0.6839"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 61/100 [=================>............] - ETA: 12s - loss: 0.7484 - accuracy: 0.6834"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 46s 457ms/step - loss: 0.7303 - accuracy: 0.6913 - val_loss: 0.6510 - val_accuracy: 0.7470\n",
            "Epoch 2/5\n",
            " 89/100 [=========================>....] - ETA: 3s - loss: 0.6931 - accuracy: 0.7114"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 51s 512ms/step - loss: 0.6900 - accuracy: 0.7138 - val_loss: 0.6121 - val_accuracy: 0.7450\n",
            "Epoch 3/5\n",
            " 78/100 [======================>.......] - ETA: 7s - loss: 0.7121 - accuracy: 0.6991"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 46s 463ms/step - loss: 0.7015 - accuracy: 0.7053 - val_loss: 0.5966 - val_accuracy: 0.7580\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 45s 454ms/step - loss: 0.6995 - accuracy: 0.7066 - val_loss: 0.5996 - val_accuracy: 0.7470\n",
            "Epoch 5/5\n",
            " 34/100 [=========>....................] - ETA: 22s - loss: 0.6771 - accuracy: 0.7243"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 46s 455ms/step - loss: 0.6921 - accuracy: 0.7181 - val_loss: 0.6074 - val_accuracy: 0.7540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b8ccb6eadd0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model.fit(train_data_generator, validation_data=dev_data, epochs=5,\n",
        "          callbacks=[model_checkpoint_callback])"
      ],
      "id": "y9hs1MsUCj8g"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oOlLRMyXoIU"
      },
      "id": "8oOlLRMyXoIU",
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}