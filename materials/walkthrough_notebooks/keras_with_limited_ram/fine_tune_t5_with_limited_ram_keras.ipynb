{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJmgxZ_-LcoX"
      },
      "source": [
        "## Fine-tuning a Seq2Seq model (T5) in Colab with limited RAM\n",
        "\n",
        "This notebook is an extension of the notebook [Training NLP models in Colab without running out of RAM](https://github.com/datasci-w266/2024-summer-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/keras_training_with_limited_ram.ipynb). This series focuses on how to avoid running out of memory by loading part of your data at a time while you train, and saving model checkpoints as you go. We recommend reading that earlier notebook first, which has more complete explanations of these techniques shown, but for fine-tuning a BERT model.\n",
        "\n",
        "This notebook focuses on sequence-to-sequence (encoder-decoder, text generation) models like T5, because the way you fine-tune the Huggingface pretrained versions of those models is a bit different than BERT. With T5, you use the full pre-trained model end-to-end without adding any additional layers.\n",
        "\n",
        "That said, you can still set up the training process in a similar way to how you'd set it up for BERT. This notebook is for tensorflow models, which allows you to use keras. We also have a [similar notebook for pytorch models](https://github.com/datasci-w266/2024-summer-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/fine_tune_t5_with_limited_ram_pytorch.ipynb), since some huggingface pretrained models are only available in pytorch.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2024-summer-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/fine_tune_t5_with_limited_ram_keras.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMWJ_MH1Lgsg",
        "outputId": "4bc46170-4d7c-49b7-8315-cdb8c9d5a106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.37.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gao8Q8P-edDd",
        "outputId": "ecf25494-f593-4d20-8584-8649422f483a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4fKpWjWlMJZo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VvqKWukMdtV"
      },
      "source": [
        "### Data\n",
        "\n",
        "To fine-tune T5, we'll use the dataset from the [week 6 lesson notebook](https://github.com/datasci-w266/2024-summer-main/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation.ipynb) for translating Shakespeare to modern English. You can [download the dataset here](https://github.com/cocoxu/Shakespeare), or access [the copy that is in the lesson_notebooks directory](https://github.com/datasci-w266/2024-summer-main/blob/master/materials/lesson_notebooks/train_plays-org-mod.txt) in the class git repo and then upload to your drive folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hId0nV1UMJct",
        "outputId": "ad82bdf7-ec0e-4c8f-b959-852dc859a579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rG6QmmqWLn6a"
      },
      "outputs": [],
      "source": [
        "# Modify this path to where you saved the Shakespear data in your Drive\n",
        "text_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_plays-org-mod.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t_i8h2C2Mcyd"
      },
      "outputs": [],
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "\n",
        "prefix = 'translate old to modern: '\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    orig, target = line.split('\\t')\n",
        "    text_pairs.append({'orig': prefix + orig, 'target': target})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDHlRNR0Mc1D",
        "outputId": "73dc0dcc-beeb-4b39-99e2-6dcf7c86bcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'orig': \"translate old to modern: Think what thou wilt, I am thy lover's grace; And like Limander am I trusty still.\", 'target': \"Think what you will, I am your lover's grace; And like Limander, I am still trusty.\"}\n",
            "{'orig': 'translate old to modern: Your brother is but young and tender, and, for your love I would be loath to foil him, as I must for my own honor if he come in.', 'target': 'Your brother is young and inexperienced, and because of my affection for you, I’d hate to crush him—though I’d have to, if he challenged me.'}\n",
            "{'orig': 'translate old to modern: Give me some help.', 'target': 'Give me some help.'}\n",
            "{'orig': 'translate old to modern: I had rather hear my dog bark at a crow than a man swear he loves me.', 'target': 'I would rather listen to my dog bark at a crow than hear a man swear that he loves me.'}\n",
            "{'orig': 'translate old to modern: I will ask him for my place again; he shall tell me I am a drunkard!', 'target': 'I’ll ask him for my job back; he’ll tell me I am a drunkard!'}\n"
          ]
        }
      ],
      "source": [
        "# Look at some examples\n",
        "for _ in range(5):\n",
        "    print(np.random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGbY61KYMlWo",
        "outputId": "a39190e3-74f3-4bfc-9db7-02f396ac41af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19088 total pairs\n",
            "13362 training pairs\n",
            "2863 validation pairs\n",
            "2863 test pairs\n"
          ]
        }
      ],
      "source": [
        "# Let's create some splits\n",
        "np.random.shuffle(text_pairs)\n",
        "num_valid_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "valid_pairs = text_pairs[num_train_samples : num_train_samples + num_valid_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_valid_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(valid_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B3UO4xczMlZe"
      },
      "outputs": [],
      "source": [
        "# Save splits to separate csv files, to load only part at a time later\n",
        "train_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_pairs.csv'\n",
        "valid_file = 'drive/MyDrive/ISchool/MIDS/266/data/valid_pairs.csv'\n",
        "test_file = 'drive/MyDrive/ISchool/MIDS/266/data/test_pairs.csv'\n",
        "\n",
        "pd.DataFrame(train_pairs).to_csv(train_file)\n",
        "pd.DataFrame(valid_pairs).to_csv(valid_file)\n",
        "pd.DataFrame(test_pairs).to_csv(test_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9aloMo7QnBG"
      },
      "source": [
        "### Preprocessor and Data Generator\n",
        "\n",
        "As in the earlier notebook for BERT models, we'll define a preprocessing function that takes a tokenizer and one batch of text data, tokenizes the text and returns the inputs to the model (input vocab ids, input attention mask, and output vocab ids as labels).\n",
        "\n",
        "Then we'll define a data generator class that will load one batch of data from file every time keras gets a new batch for training. This way, we don't load all of our data into memory at once. The data generator will call the preprocessing function, returning a list of model inputs plus the labels.\n",
        "\n",
        "For a seq2seq model, we'll not only pass in the input_ids and attention_mask for the encoder (original text), we'll also need to pass in the decoder_input_ids (vocab ids for the output text). The T5 model has a handy function to shift the output vocab ids (i.e. the labels) over by one, so they start with the starter token for the decoder inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZiSOsXovGwb7"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(text_pairs, tokenizer, model, max_length=128):\n",
        "    orig_text = [orig for orig, target in text_pairs]\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        orig_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = np.array(orig_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    orig_attention_masks = np.array(orig_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    target_text = [target for orig, target in text_pairs]\n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        target_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(target_encoded['input_ids'])\n",
        "    decoder_input_ids = model._shift_right(label_ids)\n",
        "\n",
        "    return [orig_input_ids, orig_attention_masks, decoder_input_ids], label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZoW-A-ENSJ3J"
      },
      "outputs": [],
      "source": [
        "class TranslationDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "\n",
        "        text_pairs = df[['orig', 'target']].values.astype(str).tolist()\n",
        "\n",
        "        batch_data = preprocess_data(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.model,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P9QZXI911qA"
      },
      "source": [
        "### Pretrained model\n",
        "\n",
        "Huggingface's pretrained tensorflow models are keras models, so you could call .compile() and .fit() directly on the pre-trained T5 model. But for sequence-to-sequence models, it can be tricky to make sure the right inputs are going into the right part of the model (encoder vs decoder, etc).\n",
        "\n",
        "Even though we aren't adding any other layers, we can still create a keras model wrapper around the pretrained T5 model. That way, we can pass in the right inputs into the model using keyword arguments.\n",
        "\n",
        "We'll use the first output of the T5 model (the logits for the output vocab) as the output of the overall model, and compile with crossentropy loss. Then we can call .fit on the wrapper model like we did in the last notebook, passing in the data generators for train and validation data instead of a fully loaded dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfznBi6kBM6z",
        "outputId": "826bdbe9-d4fd-4ace-c89e-a7befe8965a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained tensorflow model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WExN1Fp3SJ5c"
      },
      "outputs": [],
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 32\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZacUWKNwrMcU"
      },
      "outputs": [],
      "source": [
        "def build_t5_training_wrapper_model(t5_model, max_length):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='labels')\n",
        "\n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZXRCuarjrMen"
      },
      "outputs": [],
      "source": [
        "model_wrapper = build_t5_training_wrapper_model(t5_model, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VDh-uMYG8jLe"
      },
      "outputs": [],
      "source": [
        "# As in the first notebook, we should add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch. Edit the filepath to where\n",
        "# you want to save the weights in your own Drive\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KkhVMODrMg9",
        "outputId": "87795a4a-957a-4a7b-b761-8182077f4c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "835/835 [==============================] - 384s 378ms/step - loss: 0.8244 - accuracy: 0.8405 - val_loss: 0.7603 - val_accuracy: 0.8513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fcb5028a4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Now call .fit on the model_wrapper, passing in the data generators and the\n",
        "# model checkpoint callback\n",
        "\n",
        "model_wrapper.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=1,\n",
        "                  callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9NGPOrhUbRC"
      },
      "source": [
        "### Does it work?\n",
        "\n",
        "Depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned T5 for this new task we defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ky4-uaxGweg",
        "outputId": "5488a716-8480-46b4-f550-f158f3863ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You won’t vex me again.']\n",
            "['Do you want to see me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ],
      "source": [
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GzuZEB6EUa7f"
      },
      "outputs": [],
      "source": [
        "# To pick back up where you left off, load the saved model weights\n",
        "# (Edit the filename to the last saved one that you want to load)\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.01-0.85.hdf5'\n",
        "model_wrapper.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFxYoN-kC133",
        "outputId": "0b90fe42-580d-4b16-fecd-92d1d03ff152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You won’t vex me again.']\n",
            "['Do you want to see me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ],
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55l1GRNtC5P8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}